{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-bigquery --quiet"
      ],
      "metadata": {
        "id": "p6ItS33oH_7A"
      },
      "id": "p6ItS33oH_7A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "I4iKK155gQa9NhDAaNQFVMuH",
      "metadata": {
        "tags": [],
        "id": "I4iKK155gQa9NhDAaNQFVMuH"
      },
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "# Set the project ID explicitly if the client doesn't pick it up automatically\n",
        "PROJECT_ID = 'stgw-shared-ai-dev'\n",
        "\n",
        "# The BigQuery client should automatically use the Service Account\n",
        "# associated with your Colab Enterprise notebook/runtime.\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "print(f\"BigQuery Client initialized for project: {PROJECT_ID}\")\n",
        "# You can now proceed to run your BigQuery queries (CREATE TABLE, SELECT, etc.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "client = bigquery.Client()\n",
        "table_id = \"stgw-shared-ai-dev.parquet_data_set.IMP_profile_joined_behavioral_base\"\n",
        "\n",
        "# Query BigQuery's information schema to get column names\n",
        "query_columns = f\"\"\"\n",
        "SELECT column_name\n",
        "FROM `{table_id.split('.')[0]}`.{table_id.split('.')[1]}.INFORMATION_SCHEMA.COLUMNS\n",
        "WHERE table_name = '{table_id.split('.')[-1]}'\n",
        "\"\"\"\n",
        "\n",
        "columns = [row['column_name'] for row in client.query(query_columns).result()]\n",
        "\n",
        "print (f\"Fetched {len(columns)} column names successfully.\")"
      ],
      "metadata": {
        "id": "_Sai8trxFXmG"
      },
      "id": "_Sai8trxFXmG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Missing Percentage calculation"
      ],
      "metadata": {
        "id": "iRm0bE722nt-"
      },
      "id": "iRm0bE722nt-"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "# Initialize client\n",
        "client = bigquery.Client(project=\"stgw-shared-ai-dev\")\n",
        "\n",
        "# --- PARAMETERS ---\n",
        "dataset = \"parquet_data_set\"\n",
        "table = \"IMP_profile_joined_behavioral_base\"\n",
        "dataset_ref = f\"{client.project}.{dataset}\"\n",
        "\n",
        "# --- STEP 1: Get column metadata ---\n",
        "query_columns = f\"\"\"\n",
        "SELECT column_name, data_type, ordinal_position\n",
        "FROM `{dataset_ref}.INFORMATION_SCHEMA.COLUMNS`\n",
        "WHERE table_name = '{table}'\n",
        "ORDER BY ordinal_position\n",
        "\"\"\"\n",
        "columns = client.query(query_columns).to_dataframe()\n",
        "\n",
        "# --- STEP 2: Build COUNTIF expressions ---\n",
        "countif_clauses = []\n",
        "for _, row in columns.iterrows():\n",
        "    col, dtype = row[\"column_name\"], row[\"data_type\"]\n",
        "    if dtype == \"STRING\":\n",
        "        expr = f\"COUNTIF({col} IS NULL OR TRIM({col}) = '') AS missing_{col}\"\n",
        "    else:\n",
        "        expr = f\"COUNTIF({col} IS NULL) AS missing_{col}\"\n",
        "    countif_clauses.append(expr)\n",
        "\n",
        "countif_clause_str = \",\\n        \".join(countif_clauses)\n",
        "\n",
        "# --- STEP 3: Build union (unpivot) statements ---\n",
        "union_clauses = []\n",
        "for _, row in columns.iterrows():\n",
        "    col, dtype = row[\"column_name\"], row[\"data_type\"]\n",
        "    union_clauses.append(\n",
        "        f\"SELECT '{col}' AS column_name, '{dtype}' AS data_type, \"\n",
        "        f\"(missing_{col} * 100.0 / total_rows) AS missing_percentage FROM MissingnessData\"\n",
        "    )\n",
        "union_clause_str = \"\\nUNION ALL\\n\".join(union_clauses)\n",
        "\n",
        "# --- STEP 4: Assemble final query ---\n",
        "final_query = f\"\"\"\n",
        "WITH MissingnessData AS (\n",
        "    SELECT\n",
        "        COUNT(*) AS total_rows,\n",
        "        {countif_clause_str}\n",
        "    FROM `{dataset_ref}.{table}`\n",
        ")\n",
        "{union_clause_str}\n",
        "ORDER BY missing_percentage DESC\n",
        "\"\"\"\n",
        "\n",
        "print(\"‚úÖ Query built successfully!\\n\")\n",
        "\n",
        "# --- STEP 5: Run the query ---\n",
        "df_result = client.query(final_query).to_dataframe()\n",
        "df_result.head(20)  # Display top 20 columns with missingness\n"
      ],
      "metadata": {
        "id": "ivXYDpet07Mn"
      },
      "id": "ivXYDpet07Mn",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# dataframe: df_result\n",
        "# uuid: A4B49738-F56C-4CE8-8BE1-8FD427D51CAE\n",
        "# output_variable:\n",
        "\n",
        "import google.colabsqlviz.explore_dataframe as _vizcell\n",
        "_vizcell.explore_dataframe(df_or_df_name='df_result', uuid='A4B49738-F56C-4CE8-8BE1-8FD427D51CAE')"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "colab_type": "viz",
        "id": "u5Wu88o4-nQe"
      },
      "id": "u5Wu88o4-nQe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Column Profiling"
      ],
      "metadata": {
        "id": "AJVEJc6QC52W"
      },
      "id": "AJVEJc6QC52W"
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß© What the script does\n",
        "\n",
        "It queries your BigQuery table column by column (one at a time).\n",
        "\n",
        "For each column, it computes:\n",
        "\n",
        "total number of rows\n",
        "\n",
        "number of unique values\n",
        "\n",
        "% of unique values (relative to total rows)\n",
        "\n",
        "top 3 most frequent values with counts (top_values)\n",
        "\n",
        "an imbalance ratio (share of the most frequent value)\n",
        "\n",
        "It then appends each column‚Äôs profile into one combined DataFrame ‚Üí\n",
        "‚úÖ 227 rows total (one per column)"
      ],
      "metadata": {
        "id": "FVZbOB1hQwwG"
      },
      "id": "FVZbOB1hQwwG"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-bigquery --quiet\n",
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "client = bigquery.Client(project=\"stgw-shared-ai-dev\")\n",
        "\n",
        "dataset = \"parquet_data_set\"\n",
        "table = \"IMP_profile_joined_behavioral_base\"\n",
        "dataset_ref = f\"{client.project}.{dataset}\"\n",
        "\n",
        "# --- 1Ô∏è‚É£ Get column list ---\n",
        "query_columns = f\"\"\"\n",
        "SELECT column_name, data_type\n",
        "FROM `{dataset_ref}.INFORMATION_SCHEMA.COLUMNS`\n",
        "WHERE table_name = '{table}'\n",
        "ORDER BY ordinal_position\n",
        "\"\"\"\n",
        "cols = client.query(query_columns).to_dataframe()\n",
        "\n",
        "# --- 2Ô∏è‚É£ Define helper to quote column safely ---\n",
        "def safe(col):\n",
        "    return f\"`{col.replace('`','')}`\"\n",
        "\n",
        "# --- 3Ô∏è‚É£ Build and run per-column query safely ---\n",
        "profiles = []\n",
        "for _, row in cols.iterrows():\n",
        "    col, dtype = row[\"column_name\"], row[\"data_type\"]\n",
        "    cq = safe(col)\n",
        "\n",
        "    # build small subquery\n",
        "    q = f\"\"\"\n",
        "    SELECT\n",
        "      '{col}' AS column_name,\n",
        "      '{dtype}' AS data_type,\n",
        "      COUNT(*) AS total_rows,\n",
        "      COUNT(DISTINCT {cq}) AS unique_values,\n",
        "      ROUND(COUNT(DISTINCT {cq}) * 100.0 / COUNT(*), 2) AS uniqueness_pct,\n",
        "      (\n",
        "        SELECT ARRAY_AGG(STRUCT(val, freq) ORDER BY freq DESC LIMIT 3)\n",
        "        FROM (\n",
        "          SELECT {cq} AS val, COUNT(*) AS freq\n",
        "          FROM `{dataset_ref}.{table}`\n",
        "          WHERE {cq} IS NOT NULL\n",
        "          GROUP BY {cq}\n",
        "          ORDER BY freq DESC\n",
        "          LIMIT 3\n",
        "        )\n",
        "      ) AS top_values,\n",
        "      (\n",
        "        SELECT MAX(freq) / SUM(freq)\n",
        "        FROM (\n",
        "          SELECT {cq} AS val, COUNT(*) AS freq\n",
        "          FROM `{dataset_ref}.{table}`\n",
        "          WHERE {cq} IS NOT NULL\n",
        "          GROUP BY {cq}\n",
        "        )\n",
        "      ) AS imbalance_ratio\n",
        "    FROM `{dataset_ref}.{table}`\n",
        "    LIMIT 1\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        subdf = client.query(q).to_dataframe()\n",
        "        profiles.append(subdf)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Skipping column '{col}' due to SQL error: {e}\")\n",
        "        continue\n",
        "\n",
        "# --- 4Ô∏è‚É£ Combine results ---\n",
        "if profiles:\n",
        "    df_profile = pd.concat(profiles, ignore_index=True)\n",
        "else:\n",
        "    raise RuntimeError(\"No valid column profiles generated.\")\n",
        "\n",
        "# --- 5Ô∏è‚É£ Recommendations ---\n",
        "def rec(row):\n",
        "    if row.unique_values <= 1:\n",
        "        return \"‚ö†Ô∏è Constant ‚Äî drop\"\n",
        "    if row.unique_values < 10:\n",
        "        return \"‚úÖ Good categorical\"\n",
        "    if row.unique_values < 50:\n",
        "        return \"‚ÑπÔ∏è Moderate ‚Äî check imbalance\"\n",
        "    if row.unique_values < 200:\n",
        "        return \"üß© Consider binning/grouping\"\n",
        "    return \"üìâ Continuous / high-cardinality\"\n",
        "\n",
        "df_profile[\"recommendation\"] = df_profile.apply(rec, axis=1)\n",
        "\n",
        "# --- 6Ô∏è‚É£ Display ---\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "df_profile[[\n",
        "    \"column_name\", \"data_type\", \"unique_values\",\n",
        "    \"uniqueness_pct\", \"imbalance_ratio\", \"recommendation\", \"top_values\"\n",
        "]].head(30)\n"
      ],
      "metadata": {
        "id": "Y4OJiI2aDBF4"
      },
      "id": "Y4OJiI2aDBF4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_profile)\n"
      ],
      "metadata": {
        "id": "Gt4nPDhzQ9cU"
      },
      "id": "Gt4nPDhzQ9cU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing = set(cols[\"column_name\"]) - set(df_profile[\"column_name\"])\n",
        "missing\n"
      ],
      "metadata": {
        "id": "BWRHggC_RDde"
      },
      "id": "BWRHggC_RDde",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 7Ô∏è‚É£: Save and download results in Colab ---\n",
        "\n",
        "from google.colab import files\n",
        "import datetime\n",
        "\n",
        "# Create a timestamped filename (optional but helps versioning)\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "filename = f\"column_profiling_results_{timestamp}.csv\"\n",
        "\n",
        "# Save to CSV\n",
        "df_profile.to_csv(filename, index=False)\n",
        "\n",
        "# Notify and trigger download\n",
        "print(f\"‚úÖ File saved as {filename}\")\n",
        "files.download(filename)\n"
      ],
      "metadata": {
        "id": "Q-kuPpj5RXxB"
      },
      "id": "Q-kuPpj5RXxB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imputation and Missing"
      ],
      "metadata": {
        "id": "-HOM6m9lKH2O"
      },
      "id": "-HOM6m9lKH2O"
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Column Type | What happens in your code                       | Notes / Limitations                                                       |\n",
        "| ----------- | ----------------------------------------------- | ------------------------------------------------------------------------- |\n",
        "| Categorical | `NULL` ‚Üí `\"__MISSING__\"`                        | Good for placeholders, preserves missing info                             |\n",
        "| Numeric     | `NULL` ‚Üí `\"__MISSING__\"` (converted to string!) | ‚ö†Ô∏è Numeric info is lost; you probably want median/mean imputation instead |\n",
        "| All columns | `_is_missing` flag created                      | Useful for autoencoder to learn missing patterns                          |\n"
      ],
      "metadata": {
        "id": "58OY-zsDbpSI"
      },
      "id": "58OY-zsDbpSI"
    },
    {
      "cell_type": "code",
      "source": [
        "select_clauses = [\"*\"] # Start with all original columns\n",
        "\n",
        "for col in columns:\n",
        "    # 1. Impute NULLs with a placeholder string '__MISSING__'\n",
        "    imputation_clause = f\"COALESCE(CAST({col} AS STRING), '__MISSING__') AS {col}_imputed\"\n",
        "\n",
        "    # 2. Create the missing flag column\n",
        "    missing_flag_clause = f\"CASE WHEN {col} IS NULL THEN 1 ELSE 0 END AS {col}_is_missing\"\n",
        "\n",
        "    select_clauses.append(imputation_clause)\n",
        "    select_clauses.append(missing_flag_clause)\n",
        "\n",
        "# Combine all clauses into the final SELECT part\n",
        "select_statement = \",\\n\".join(select_clauses)\n",
        "\n",
        "print(\"SELECT statement generated successfully.\")"
      ],
      "metadata": {
        "id": "46SK2EhAJueq"
      },
      "id": "46SK2EhAJueq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_sql_query = f\"\"\"\n",
        "CREATE OR REPLACE TABLE `stgw-shared-ai-dev.parquet_data_set.IMP_profile_FE_step_2` AS\n",
        "SELECT\n",
        "    {select_statement}\n",
        "FROM\n",
        "    `{table_id}`\n",
        "\"\"\"\n",
        "\n",
        "print(\"Starting BigQuery Feature Engineering Job...\")\n",
        "# Execute the query (this will be a very large job run in BigQuery)\n",
        "job = client.query(final_sql_query)\n",
        "# Wait for the job to complete (you are waiting for the job to finish on the server)\n",
        "job.result()\n",
        "\n",
        "print(f\"Feature engineering table created successfully: {job.destination.table_id}\")"
      ],
      "metadata": {
        "id": "_YGA0by2KU_G"
      },
      "id": "_YGA0by2KU_G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sample and Extract Model Features**"
      ],
      "metadata": {
        "id": "i56sqaM9cuW8"
      },
      "id": "i56sqaM9cuW8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Step | Purpose                                              | Outcome                                                            |\n",
        "| ---- | ---------------------------------------------------- | ------------------------------------------------------------------ |\n",
        "| 1    | Decide which columns to use and drop irrelevant ones | Two lists: `_imputed` features + `_is_missing` flags               |\n",
        "| 2    | Build a SQL query selecting only relevant features   | `sample_query` that samples ~5% of rows                            |\n",
        "| 3    | Execute the query and save results                   | BigQuery table `IMP_profile_FE_new_sample` ready for preprocessing |\n"
      ],
      "metadata": {
        "id": "bOsPZWE_rzqs"
      },
      "id": "bOsPZWE_rzqs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Load Profiling Results and Determine Columns**\n",
        "\n",
        "Goal: Decide which columns to keep for modeling (drop IDs/constants, keep _imputed and _is_missing columns)."
      ],
      "metadata": {
        "id": "jRzhXT8kkkD7"
      },
      "id": "jRzhXT8kkkD7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "Load profiling results:\n",
        "\n",
        "The CSV column_profiling_results_*.csv contains analysis of all columns, such as missing value percentages, uniqueness, and recommendations.\n",
        "\n",
        "Drop unwanted columns:\n",
        "\n",
        "Columns like IDs or constants are not informative for training an autoencoder.\n",
        "\n",
        "drop_cols stores these column names so they won‚Äôt be used as features.\n",
        "\n",
        "Create feature lists:\n",
        "\n",
        "feature_cols: keeps the imputed versions of the columns (_imputed). These are your main features with nulls handled.\n",
        "\n",
        "missing_flag_cols: keeps the missing value indicator flags (_is_missing) which help the autoencoder detect patterns of missingness.\n",
        "\n",
        "Outcome: You now have two lists of columns that are safe and meaningful for training."
      ],
      "metadata": {
        "id": "ROmDtKLqq_9n"
      },
      "id": "ROmDtKLqq_9n"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Loaded profiling results ‚Äì the CSV contains each\n",
        "\n",
        "* column and whether it should be kept or dropped (like IDs or constants).\n",
        "\n",
        "* Identified columns to drop ‚Äì any column marked as an ID or constant in the recommendation is removed.\n",
        "\n",
        "* Prepared final feature lists ‚Äì for the remaining columns:\n",
        "\n",
        "* _imputed columns (values after imputation) are kept as feature_cols.\n",
        "\n",
        "* _is_missing columns (missing value flags) are kept as missing_flag_cols.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zIeMCQyR6tv0"
      },
      "id": "zIeMCQyR6tv0"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load profiling results (generated from your earlier profiling step)\n",
        "profile = pd.read_csv(\"/content/column_profiling_results_20251025_072916.csv\")\n",
        "\n",
        "# Identify columns to drop (IDs, constants)\n",
        "drop_cols = profile.loc[\n",
        "    profile[\"recommendation\"].str.contains(\"drop|id\", case=False, na=False),\n",
        "    \"column_name\"\n",
        "].tolist()\n",
        "\n",
        "# Keep imputed columns\n",
        "feature_cols = [f\"{col}_imputed\" for col in profile[\"column_name\"] if col not in drop_cols]\n",
        "\n",
        "# Keep missing flags\n",
        "missing_flag_cols = [f\"{col}_is_missing\" for col in profile[\"column_name\"] if col not in drop_cols]\n"
      ],
      "metadata": {
        "id": "45ZQudFHkmSZ"
      },
      "id": "45ZQudFHkmSZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Build a Random Sample Query**\n",
        "\n",
        "Goal: Select only the relevant columns from IMP_profile_FE_step_2 and sample a fraction (to keep Colab memory safe)."
      ],
      "metadata": {
        "id": "d1CHdglYlBzD"
      },
      "id": "d1CHdglYlBzD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "Combine columns to select:\n",
        "\n",
        "select_columns includes all _imputed features + _is_missing flags.\n",
        "\n",
        "Generate SQL SELECT statement:\n",
        "\n",
        "select_statement is a comma-separated string of all columns to include in the query.\n",
        "\n",
        "Random sampling:\n",
        "\n",
        "WHERE RAND() < 0.05 selects approximately 5% of rows randomly.\n",
        "\n",
        "This keeps the dataset manageable in memory while preserving variability for training.\n",
        "\n",
        "Outcome: You have a SQL query (sample_query) that selects only the relevant columns and samples a small subset of rows."
      ],
      "metadata": {
        "id": "zdawjRRjrBzi"
      },
      "id": "zdawjRRjrBzi"
    },
    {
      "cell_type": "code",
      "source": [
        "select_columns = feature_cols + missing_flag_cols\n",
        "select_statement = \", \".join(select_columns)\n",
        "\n",
        "sample_query = f\"\"\"\n",
        "SELECT {select_statement}\n",
        "FROM `stgw-shared-ai-dev.parquet_data_set.IMP_profile_FE_step_2`\n",
        "WHERE RAND() < 0.05  -- ~5% random sample\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "ofdf_OKtlDsJ"
      },
      "id": "ofdf_OKtlDsJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Write Sample to a BigQuery Table**\n",
        "\n",
        "Goal: Avoid ‚Äúresponse too large‚Äù errors by storing the sample in a new BigQuery table first."
      ],
      "metadata": {
        "id": "NodCgpB3lvkN"
      },
      "id": "NodCgpB3lvkN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "Initialize BigQuery client:\n",
        "\n",
        "client = bigquery.Client(...) lets you run queries and manage tables in BigQuery.\n",
        "\n",
        "Define destination table:\n",
        "\n",
        "sample_table_id is the new table that will hold the sampled data.\n",
        "\n",
        "WRITE_TRUNCATE ensures any existing table with the same name is replaced.\n",
        "\n",
        "Run the query and save results:\n",
        "\n",
        "client.query(..., job_config=job_config) executes the SQL sample_query in BigQuery.\n",
        "\n",
        "job.result() waits for the query to finish.\n",
        "\n",
        "Outcome:\n",
        "\n",
        "BigQuery now has a sampled, clean, feature-ready table called IMP_profile_FE_new_sample.\n",
        "\n",
        "This table contains only numeric-ready or imputed columns and missing value flags, ready for further preprocessing or autoencoder training."
      ],
      "metadata": {
        "id": "HrjBQpHHrSSj"
      },
      "id": "HrjBQpHHrSSj"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "client = bigquery.Client(project=\"stgw-shared-ai-dev\")\n",
        "sample_table_id = \"stgw-shared-ai-dev.parquet_data_set.IMP_profile_FE_new_sample\"\n",
        "\n",
        "job_config = bigquery.QueryJobConfig(destination=sample_table_id)\n",
        "job_config.write_disposition = \"WRITE_TRUNCATE\"  # overwrite if exists\n",
        "\n",
        "job = client.query(sample_query, job_config=job_config)\n",
        "job.result()  # wait for the query to finish\n",
        "\n",
        "print(f\"Sample written to table: {sample_table_id}\")\n"
      ],
      "metadata": {
        "id": "z23NH8vZlxz9"
      },
      "id": "z23NH8vZlxz9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BREAK**"
      ],
      "metadata": {
        "id": "FAxo2QJ2rm8q"
      },
      "id": "FAxo2QJ2rm8q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Visualisation**"
      ],
      "metadata": {
        "id": "Azmy0-ExreXK"
      },
      "id": "Azmy0-ExreXK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import a Sample of the Feature-Engineered Table\n",
        "First, we must sample the massive table in BigQuery and load that sample into your Colab memory. We will use the approach defined earlier."
      ],
      "metadata": {
        "id": "I1Qafe6CMQAQ"
      },
      "id": "I1Qafe6CMQAQ"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "import numpy as np\n",
        "\n",
        "# Ensure your client is initialized for Colab Enterprise (as discussed)\n",
        "client = bigquery.Client()\n",
        "table_id = \"stgw-shared-ai-dev.parquet_data_set.IMP_profile_FE_step_2\" # Use the new table\n",
        "\n",
        "# --- Determine Sampling Parameters ---\n",
        "TARGET_SAMPLE_SIZE = 100000\n",
        "total_rows_query = f\"SELECT count(*) AS total_rows FROM `{table_id}`\"\n",
        "total_rows = client.query(total_rows_query).to_dataframe().iloc[0, 0]\n",
        "sample_probability = TARGET_SAMPLE_SIZE / total_rows\n",
        "sample_probability = min(max(sample_probability, 0.0001), 1.0)\n",
        "\n",
        "print(f\"Total Rows: {total_rows}. Sampling Probability: {sample_probability:.4f}\")\n",
        "\n",
        "# --- Execute the Sampled Query ---\n",
        "query_sampled = f\"\"\"\n",
        "SELECT *\n",
        "FROM `{table_id}`\n",
        "WHERE RAND() < {sample_probability}\n",
        "LIMIT {TARGET_SAMPLE_SIZE}\n",
        "\"\"\"\n",
        "\n",
        "df_sample = client.query(query_sampled).to_dataframe()\n",
        "\n",
        "print(f\"\\nSuccessfully imported a sample of {len(df_sample)} rows.\")\n",
        "print(f\"Sampled DataFrame shape: {df_sample.shape}\")"
      ],
      "metadata": {
        "id": "BN7fnacUMGKl"
      },
      "id": "BN7fnacUMGKl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Visualization of Missingness (Binary Features)\n",
        "This plot validates your *_is_missing columns and shows which original features had the highest rates of missing data."
      ],
      "metadata": {
        "id": "javnOwfjMa8h"
      },
      "id": "javnOwfjMa8h"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Identify all the binary missing flag columns\n",
        "missing_cols = [col for col in df_sample.columns if col.endswith('_is_missing')]\n",
        "\n",
        "# Calculate the mean of these columns (which gives the proportion of missing values)\n",
        "missing_rate = df_sample[missing_cols].mean().sort_values(ascending=False)\n",
        "\n",
        "# Plot the top 20 most missing features\n",
        "plt.figure(figsize=(12, 6))\n",
        "missing_rate.head(20).plot(kind='bar', color='skyblue')\n",
        "plt.title('Top 20 Features by Missing Value Rate')\n",
        "plt.ylabel('Proportion Missing (0 to 1)')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show() #\n",
        "\n",
        "print(\"Visualized the proportion of missing data in the original features.\")"
      ],
      "metadata": {
        "id": "7zbdG-EDMpo5"
      },
      "id": "7zbdG-EDMpo5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspecting the Sampled Columns\n",
        "Run this code in Colab to see the names of all the new feature-engineered columns in your df_sample DataFrame:"
      ],
      "metadata": {
        "id": "Tw6BahdfMxOD"
      },
      "id": "Tw6BahdfMxOD"
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify the two types of feature-engineered columns\n",
        "imputed_cols = sorted([col for col in df_sample.columns if col.endswith('_imputed')])\n",
        "missing_flag_cols = sorted([col for col in df_sample.columns if col.endswith('_is_missing')])\n",
        "\n",
        "print(\"--- 1. Imputed Columns (String/Categorical) ---\")\n",
        "print(f\"Total: {len(imputed_cols)}\")\n",
        "# Print the first 10 for a quick look\n",
        "print(imputed_cols[:10])\n",
        "\n",
        "print(\"\\n--- 2. Missing Flag Columns (Binary 0/1) ---\")\n",
        "print(f\"Total: {len(missing_flag_cols)}\")\n",
        "# Print the first 10 for a quick look\n",
        "print(missing_flag_cols[:10])"
      ],
      "metadata": {
        "id": "vaqDGNRHMydh"
      },
      "id": "vaqDGNRHMydh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choosing and Visualizing Columns\n",
        "Once you've reviewed the lists above, you can replace the placeholder names in sample_imputed_cols with actual columns that are most important or that showed high missingness in the previous step (e.g., from the \"Top 20 Features by Missing Value Rate\" chart)."
      ],
      "metadata": {
        "id": "rh7bBvl7NaXH"
      },
      "id": "rh7bBvl7NaXH"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Replace these with actual column names you chose from the lists above\n",
        "sample_imputed_cols = [\n",
        "    'activities_imputed',\n",
        "    'adults_income_imputed', # Replace with an actual imputed column from your list\n",
        "    'age_imputed'    # Replace with another actual imputed column\n",
        "]\n",
        "\n",
        "for col in sample_imputed_cols:\n",
        "    if col in df_sample.columns:\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        # Plot the top 10 value counts, including the '__MISSING__' placeholder\n",
        "        df_sample[col].value_counts().head(10).plot(kind='barh', color='lightcoral')\n",
        "        plt.title(f'Distribution for {col}')\n",
        "        plt.show() #\n",
        "    else:\n",
        "        print(f\"Warning: Column '{col}' not found in the sample DataFrame.\")"
      ],
      "metadata": {
        "id": "kxkIllNZNbp6"
      },
      "id": "kxkIllNZNbp6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Correlation of Missingness Patterns\n",
        "This plot validates the relationships between your new binary features. It shows if some features tend to be missing at the same time."
      ],
      "metadata": {
        "id": "JyTFFoi_N03_"
      },
      "id": "JyTFFoi_N03_"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame only with the missing flag columns\n",
        "df_missing_flags = df_sample[missing_cols]\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "# Due to the high number of columns (227), we sample a subset for the heatmap\n",
        "sample_cols = df_missing_flags.sample(n=30, axis=1, random_state=42)\n",
        "corr_matrix_missing = sample_cols.corr()\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "sns.heatmap(corr_matrix_missing, cmap='viridis', annot=False, linewidths=.5, cbar_kws={'label': 'Correlation (0 to 1)'})\n",
        "plt.title('Correlation Heatmap of Sampled Missingness Flags (0=No Correlation, 1=Missing Together)')\n",
        "plt.show() #\n",
        "\n",
        "print(\"Visualized patterns in features missing simultaneously.\")"
      ],
      "metadata": {
        "id": "Y94dnicoN2KC"
      },
      "id": "Y94dnicoN2KC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Feature Matrix Creation and Dimensionality Reduction (BigQuery ML)Goal:**\n",
        "Convert $\\approx 681$ high-cardinality categorical features into a small, dense, numeric feature matrix. This is the most crucial step for clustering categorical data.A. Dimensionality Reduction via Feature EmbeddingsThe most effective method for \"latent behavioral patterns\" is to use Feature Embeddings via a BQML model like Autoencoder or Matrix Factorization. This converts your wide categorical table into a narrow numeric table."
      ],
      "metadata": {
        "id": "uN4mk99_KvcV"
      },
      "id": "uN4mk99_KvcV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Revised BQML Query: Implementing Sampling (Recommended)\n",
        "You can sample the data directly within the BigQuery SQL query using the WHERE RAND() < X clause. A small sample (e.g., 1 million rows, or about 0.4% of your data) is often sufficient for initial model training and evaluation."
      ],
      "metadata": {
        "id": "PrDHXBlsmxG_"
      },
      "id": "PrDHXBlsmxG_"
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from google.cloud import bigquery\n",
        "from google.cloud.bigquery import QueryJobConfig # Imported but not strictly needed for this simple query\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 0. Setup and Query Definition\n",
        "# -------------------------------------------------------------\n",
        "PROJECT_ID = 'stgw-shared-ai-dev'\n",
        "LOCATION = 'us-west1' # Using us-west1 to match the location error encountered previously\n",
        "\n",
        "# Initialize the BigQuery client\n",
        "client = bigquery.Client(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "# Use a sampling rate (e.g., 0.005 = 0.5% of 261M rows ‚âà 1.3 million rows)\n",
        "SAMPLING_RATE = 0.005\n",
        "\n",
        "bqml_query = f\"\"\"\n",
        "CREATE OR REPLACE MODEL\n",
        "  `stgw-shared-ai-dev.parquet_data_set.behavioral_autoencoder_SAMPLE`\n",
        "OPTIONS(\n",
        "  model_type='AUTOENCODER',\n",
        "  HIDDEN_UNITS=[100, 50, 100],\n",
        "  OPTIMIZER='ADAM',\n",
        "  MAX_ITERATIONS=3\n",
        ") AS\n",
        "SELECT\n",
        "    * EXCEPT(\n",
        "    extern_tuid,       -- ID column\n",
        "    created_date,      -- Unsupported TIMESTAMP type\n",
        "    dateOfBirth        -- Unsupported DATE/TIMESTAMP type\n",
        ")\n",
        "FROM\n",
        "    `stgw-shared-ai-dev.parquet_data_set.IMP_profile_FE_step_2`\n",
        "WHERE\n",
        "    RAND() < {SAMPLING_RATE}  -- Implementing the sampling\n",
        "\"\"\"\n",
        "\n",
        "print(f\"Starting sampled BQML Autoencoder Training Job in {LOCATION}...\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 1. Start the BigQuery job\n",
        "# -------------------------------------------------------------\n",
        "job = client.query(bqml_query)\n",
        "print(f\"Job started successfully. ID: {job.job_id}\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 2. Asynchronous Polling Loop (Mitigation for Runtime Disconnects)\n",
        "# -------------------------------------------------------------\n",
        "MAX_POLL_TIME_MINUTES = 360 # 6 hours max duration\n",
        "poll_interval_seconds = 600 # Check status every 10 minutes\n",
        "\n",
        "start_time = time.time()\n",
        "while job.state != 'DONE' and (time.time() - start_time) < (MAX_POLL_TIME_MINUTES * 60):\n",
        "    time.sleep(poll_interval_seconds)\n",
        "\n",
        "    # Reload the job status from BigQuery\n",
        "    job.reload()\n",
        "\n",
        "    elapsed = int((time.time() - start_time) / 60)\n",
        "\n",
        "    # Print status to keep the notebook active\n",
        "    print(f\"Job status: {job.state} | Elapsed: {elapsed} min. (Checking again in {poll_interval_seconds/60} min)\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 3. Final Check and Output\n",
        "# -------------------------------------------------------------\n",
        "if job.error_result:\n",
        "    # If the job failed on the BigQuery side\n",
        "    raise Exception(f\"BigQuery Job Failed: {job.error_result['reason']}. Check BQ Console for details.\")\n",
        "elif job.state == 'DONE':\n",
        "    # If the job completed successfully\n",
        "    print(f\"\\nAutoencoder Model created successfully: {job.destination.table_id} in {LOCATION}\")\n",
        "else:\n",
        "    # If the polling loop timed out before BigQuery was finished\n",
        "    print(f\"\\nPOLLING TIMEOUT: Job is still running on BigQuery but the notebook stopped waiting. Job ID: {job.job_id}\")\n",
        "    print(\"You can check the final status directly in the BigQuery UI using the Job ID.\")"
      ],
      "metadata": {
        "id": "WHmdMhuIv2Kv"
      },
      "id": "WHmdMhuIv2Kv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "from google.colab import auth\n",
        "\n",
        "# Initialize the BigQuery client (adjust location if necessary)\n",
        "PROJECT_ID = 'stgw-shared-ai-dev'\n",
        "TABLE_ID_BASE = \"stgw-shared-ai-dev.parquet_data_set.IMP_profile_joined_behavioral_base\"\n",
        "\n",
        "# Ensure client is initialized (authentication is assumed in Colab Enterprise)\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "# 1. Query BigQuery's information schema to get column names\n",
        "query_columns = f\"\"\"\n",
        "SELECT column_name\n",
        "FROM `{TABLE_ID_BASE.split('.')[0]}`.{TABLE_ID_BASE.split('.')[1]}.INFORMATION_SCHEMA.COLUMNS\n",
        "WHERE table_name = '{TABLE_ID_BASE.split('.')[-1]}'\n",
        "\"\"\"\n",
        "\n",
        "# Store the list of 227 column names in the 'columns' variable\n",
        "columns = [row['column_name'] for row in client.query(query_columns).result()]\n",
        "\n",
        "print(f\"Successfully fetched {len(columns)} original column names.\")"
      ],
      "metadata": {
        "id": "eGwxVL8t6ANh"
      },
      "id": "eGwxVL8t6ANh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'columns' is your list of 227 original column names\n",
        "# If 'columns' is not defined, you must run the initial query to fetch them first.\n",
        "\n",
        "# 1. Manually define the list of original columns if you don't have the 'columns' variable\n",
        "# Example: columns = ['age', 'country', 'feature_X', ...]\n",
        "\n",
        "clean_features = []\n",
        "# Generate the names of the clean features (approx. 681 names)\n",
        "for col in columns:\n",
        "    clean_features.append(f\"{col}_imputed\")\n",
        "    clean_features.append(f\"{col}_is_missing\")\n",
        "\n",
        "# Add the ID column, which must be the first column\n",
        "clean_features.insert(0, \"extern_tuid\")\n",
        "\n",
        "# Final comma-separated string for the SQL SELECT clause\n",
        "clean_select_statement = \",\\n\".join(clean_features)\n",
        "\n",
        "print(\"--- COPY THE TEXT BELOW AND PASTE IT INTO BIGQUERY ---\")\n",
        "print(clean_select_statement)\n",
        "print(\"-------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "iktGlxAi5tDP"
      },
      "id": "iktGlxAi5tDP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'columns' is your list of 227 original column names (without extern_tuid, created_date, dateOfBirth)\n",
        "\n",
        "clean_features = []\n",
        "# Only generate the imputed columns\n",
        "for col in columns:\n",
        "    # Cast all imputed features to STRING to avoid type mismatches\n",
        "    clean_features.append(f\"CAST({col}_imputed AS STRING) AS {col}_imputed\")\n",
        "    # Skip the _is_missing flag features for simplicity and reduction\n",
        "\n",
        "# Add the ID column, which remains as a string/integer\n",
        "clean_features.insert(0, \"extern_tuid\")\n",
        "\n",
        "# Final comma-separated string for the SQL SELECT clause\n",
        "clean_select_statement_REDUCED = \",\\n\".join(clean_features)\n",
        "\n",
        "print(\"--- COPY THIS NEW REDUCED TEXT BELOW AND PASTE IT INTO BIGQUERY ---\")\n",
        "print(clean_select_statement_REDUCED)\n",
        "print(\"-------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "FB4xa1A2rLRM"
      },
      "id": "FB4xa1A2rLRM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "# Assume 'client' is your initialized BigQuery client object\n",
        "# client = bigquery.Client()\n",
        "\n",
        "# Define the SQL query string\n",
        "preview_query = f\"\"\"\n",
        "SELECT\n",
        "    extern_tuid,\n",
        "    -- Examples of imputed string features:\n",
        "    CAST(credit_tier_imputed AS STRING) AS credit_tier_imputed,\n",
        "    CAST(family_income_decile_imputed AS STRING) AS family_income_decile_imputed,\n",
        "    CAST(timezone_imputed AS STRING) AS timezone_imputed,\n",
        "    -- Examples of imputed high-cardinality features:\n",
        "    CAST(emailAddress_imputed AS STRING) AS emailAddress_imputed,\n",
        "    CAST(i_domains_imputed AS STRING) AS i_domains_imputed,\n",
        "    -- Examples of imputed numeric features (check if they became strings):\n",
        "    CAST(age_imputed AS STRING) AS age_imputed,\n",
        "    CAST(number_of_children_imputed AS STRING) AS number_of_children_imputed,\n",
        "    -- Additional columns:\n",
        "    CAST(credit_card_details_imputed AS STRING) AS credit_card_details_imputed,\n",
        "    CAST(home_ownership_imputed AS STRING) AS home_ownership_imputed\n",
        "FROM\n",
        "    `stgw-shared-ai-dev.parquet_data_set.IMP_profile_FE_SAMPLE`\n",
        "LIMIT 10;\n",
        "\"\"\"\n",
        "\n",
        "print(\"Running preview query in BigQuery...\")\n",
        "\n",
        "# Execute the query and load the results into a pandas DataFrame\n",
        "df_preview = client.query(preview_query).to_dataframe()\n",
        "\n",
        "print(\"Preview Results (First 10 Rows):\")\n",
        "# Display the DataFrame\n",
        "display(df_preview)"
      ],
      "metadata": {
        "id": "sm2K4MvmxQEP"
      },
      "id": "sm2K4MvmxQEP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CRAMER V'S ANALYSIS**"
      ],
      "metadata": {
        "id": "8NOCzow6PIY_"
      },
      "id": "8NOCzow6PIY_"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "# Set the project ID explicitly if the client doesn't pick it up automatically\n",
        "PROJECT_ID = 'stgw-shared-ai-dev'\n",
        "\n",
        "# The BigQuery client should automatically use the Service Account\n",
        "# associated with your Colab Enterprise notebook/runtime.\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "print(f\"BigQuery Client initialized for project: {PROJECT_ID}\")\n",
        "# You can now proceed to run your BigQuery queries (CREATE TABLE, SELECT, etc.)"
      ],
      "metadata": {
        "id": "Iu5tLadPQqfW"
      },
      "id": "Iu5tLadPQqfW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from scipy.stats import chi2_contingency\n",
        "import numpy as np\n",
        "\n",
        "# Define the project and dataset for convenience\n",
        "PROJECT_ID = 'stgw-shared-ai-dev'\n",
        "DATASET_ID = 'parquet_data_set'\n",
        "SAMPLE_TABLE = 'IMP_profile_FE_SAMPLE'\n",
        "\n",
        "# --- Construct the list of imputed columns for the query ---\n",
        "# You need the list of your original 227 column names (without extern_tuid, etc.)\n",
        "# If you have the 'columns' list from previous steps, use it. Otherwise, manually list them.\n",
        "# Example:\n",
        "# imputed_cols = [f\"{col}_imputed\" for col in columns]\n",
        "# imputed_cols.insert(0, 'extern_tuid') # Add the ID column\n",
        "\n",
        "# For this example, let's manually pick a few features to demonstrate:\n",
        "imputed_cols = [\n",
        "    'mortgage_loan_imputed',\n",
        "    'credit_cards_used_most_imputed',\n",
        "    'auto_loan_imputed',\n",
        "    'Unlock_gender_imputed',\n",
        "    'credit_tier_imputed',\n",
        "    'family_income_decile_imputed',\n",
        "    'average_household_income_imputed',\n",
        "    'financial_beliefs_imputed',\n",
        "    'loan_imputed',\n",
        "    'credit_card_spend_imputed',\n",
        "    'credit_card_balance_imputed',\n",
        "    'equity_balance_imputed',\n",
        "    'family_level_of_investable_assets_imputed',\n",
        "    'net_worth_imputed',\n",
        "    'credit_cards_used_30days_imputed',\n",
        "    'card_type_imputed'\n",
        "]\n",
        "column_list_str = \",\\n\".join(imputed_cols)\n",
        "\n",
        "# Load the data\n",
        "print(\"Loading sampled data from BigQuery for correlation check...\")\n",
        "sample_load_query = f\"\"\"\n",
        "SELECT\n",
        "    {column_list_str}\n",
        "FROM\n",
        "    `{PROJECT_ID}.{DATASET_ID}.{SAMPLE_TABLE}`\n",
        "\"\"\"\n",
        "\n",
        "# The following line assumes 'client' is your BigQuery client\n",
        "df_sample = client.query(sample_load_query).to_dataframe()\n",
        "print(f\"Data loaded with {len(df_sample)} rows and {len(imputed_cols) - 1} features.\")"
      ],
      "metadata": {
        "id": "8kO06xNmy0GU"
      },
      "id": "8kO06xNmy0GU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cramer V's function**"
      ],
      "metadata": {
        "id": "POyHzasFzBZ3"
      },
      "id": "POyHzasFzBZ3"
    },
    {
      "cell_type": "code",
      "source": [
        "def cramers_v(x, y):\n",
        "    \"\"\"Calculate Cramer's V for two categorical series.\"\"\"\n",
        "    confusion_matrix = pd.crosstab(x, y)\n",
        "\n",
        "    # Chi-squared test\n",
        "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
        "\n",
        "    # Number of observations\n",
        "    n = confusion_matrix.sum().sum()\n",
        "\n",
        "    # Degrees of freedom for rows and columns\n",
        "    r, k = confusion_matrix.shape\n",
        "\n",
        "    # Calculate Cramer's V\n",
        "    phi2 = chi2 / n\n",
        "    # Bias correction (optional but recommended)\n",
        "    # The term min(k-1, r-1) is the effective degrees of freedom\n",
        "    phi2_corrected = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
        "    r_corrected = r - ((r-1)**2)/(n-1)\n",
        "    k_corrected = k - ((k-1)**2)/(n-1)\n",
        "\n",
        "    # Final Cramer's V (bias-corrected)\n",
        "    v = np.sqrt(phi2_corrected / min(r_corrected - 1, k_corrected - 1))\n",
        "\n",
        "    # Edge case: If the matrix has only one row/column (e.g., after filtering NULLs)\n",
        "    if np.isclose(min(r_corrected - 1, k_corrected - 1), 0):\n",
        "        return 1.0 # Or handle as an error/skip\n",
        "\n",
        "    return v\n",
        "\n",
        "# List of only feature columns (excluding the ID)\n",
        "feature_cols = [col for col in imputed_cols if col != 'extern_tuid']"
      ],
      "metadata": {
        "id": "vnfLbjjuzBJP"
      },
      "id": "vnfLbjjuzBJP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Calculating Cramer's V Matrix...\")\n",
        "\n",
        "# Create an empty DataFrame for the Cramer's V matrix\n",
        "cramer_matrix = pd.DataFrame(index=feature_cols, columns=feature_cols, dtype=float)\n",
        "\n",
        "for i in range(len(feature_cols)):\n",
        "    for j in range(i, len(feature_cols)):\n",
        "        col1 = feature_cols[i]\n",
        "        col2 = feature_cols[j]\n",
        "\n",
        "        # Calculate V only once (matrix is symmetric)\n",
        "        if i == j:\n",
        "            cramer_matrix.loc[col1, col2] = 1.0\n",
        "        else:\n",
        "            # We must use the .astype(str) here to ensure all inputs are treated as categories\n",
        "            v = cramers_v(df_sample[col1].astype(str), df_sample[col2].astype(str))\n",
        "            cramer_matrix.loc[col1, col2] = v\n",
        "            cramer_matrix.loc[col2, col1] = v\n",
        "\n",
        "print(\"Cramer's V Matrix (Association Strength):\")\n",
        "# display(cramer_matrix) # Uncomment this to see the full matrix"
      ],
      "metadata": {
        "id": "1QnYo-CJzMvI"
      },
      "id": "1QnYo-CJzMvI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify highly correlated pairs (V > 0.95)\n",
        "HIGH_CORRELATION_THRESHOLD = 0.95\n",
        "redundant_pairs = []\n",
        "\n",
        "# Loop through the upper triangle (excluding the diagonal)\n",
        "for i in range(len(feature_cols)):\n",
        "    for j in range(i + 1, len(feature_cols)):\n",
        "        col1 = feature_cols[i]\n",
        "        col2 = feature_cols[j]\n",
        "        v_score = cramer_matrix.loc[col1, col2]\n",
        "\n",
        "        if v_score > HIGH_CORRELATION_THRESHOLD:\n",
        "            redundant_pairs.append((v_score, col1, col2))\n",
        "\n",
        "if redundant_pairs:\n",
        "    print(\"\\nüö® Highly Redundant Feature Pairs (V > 0.95):\")\n",
        "    for score, col_a, col_b in sorted(redundant_pairs, reverse=True):\n",
        "        print(f\"  V={score:.4f}: {col_a} vs {col_b}\")\n",
        "\n",
        "    print(\"\\nRecommendation: Remove one column from each pair listed above.\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ No highly redundant feature pairs (V > 0.95) found in the sample.\")"
      ],
      "metadata": {
        "id": "gvxbSvG8zUpY"
      },
      "id": "gvxbSvG8zUpY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CREATE CRAMER V's FUNCTION ON ENTIRE DATASET**"
      ],
      "metadata": {
        "id": "mLJxHYEe0bYu"
      },
      "id": "mLJxHYEe0bYu"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from scipy.stats import chi2_contingency\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "PROJECT_ID = 'stgw-shared-ai-dev'\n",
        "DATASET_ID = 'parquet_data_set'\n",
        "SAMPLE_TABLE = 'IMP_profile_FE_SAMPLE'\n",
        "# Assume 'client' is your initialized BigQuery client object\n",
        "\n",
        "# --- 2. DYNAMICALLY RETRIEVE FEATURE LIST FROM BIGQUERY SCHEMA ---\n",
        "\n",
        "print(\"Step 1/4: Retrieving the full list of imputed columns...\")\n",
        "\n",
        "# Query the INFORMATION_SCHEMA to get all columns ending in '_imputed' + 'extern_tuid'\n",
        "schema_query = f\"\"\"\n",
        "SELECT column_name\n",
        "FROM `{PROJECT_ID}.{DATASET_ID}.INFORMATION_SCHEMA.COLUMNS`\n",
        "WHERE table_name = '{SAMPLE_TABLE}'\n",
        "  AND column_name LIKE '%_imputed'\n",
        "ORDER BY column_name\n",
        "\"\"\"\n",
        "# The 'extern_tuid' is added separately as it's the ID, not a feature\n",
        "feature_list_df = client.query(schema_query).to_dataframe()\n",
        "\n",
        "# Create the final list of features to analyze\n",
        "imputed_feature_cols = feature_list_df['column_name'].tolist()\n",
        "imputed_feature_cols.insert(0, 'extern_tuid')\n",
        "\n",
        "column_list_str = \",\\n\".join(imputed_feature_cols)\n",
        "print(f\"‚úÖ Retrieved {len(imputed_feature_cols) - 1} imputed features.\")\n",
        "\n",
        "\n",
        "# --- 3. LOAD THE FULL SAMPLED DATASET ---\n",
        "\n",
        "print(\"Step 2/4: Loading the full sampled data into DataFrame...\")\n",
        "sample_load_query = f\"\"\"\n",
        "SELECT\n",
        "    {column_list_str}\n",
        "FROM\n",
        "    `{PROJECT_ID}.{DATASET_ID}.{SAMPLE_TABLE}`\n",
        "\"\"\"\n",
        "\n",
        "# Load the data, which should be about 1.3 million rows\n",
        "df_sample = client.query(sample_load_query).to_dataframe()\n",
        "print(f\"‚úÖ Data loaded with {len(df_sample)} rows and {len(imputed_feature_cols)} columns.\")\n",
        "\n",
        "\n",
        "# --- 4. CRAMER'S V CALCULATION FUNCTION ---\n",
        "\n",
        "print(\"Step 3/4: Defining Cramer's V function...\")\n",
        "def cramers_v(x, y):\n",
        "    \"\"\"Calculate Cramer's V for two categorical series.\"\"\"\n",
        "    confusion_matrix = pd.crosstab(x, y)\n",
        "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
        "    n = confusion_matrix.sum().sum()\n",
        "    r, k = confusion_matrix.shape\n",
        "\n",
        "    # Calculate Cramer's V (with bias correction)\n",
        "    phi2 = chi2 / n\n",
        "    phi2_corrected = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
        "    r_corrected = r - ((r-1)**2)/(n-1)\n",
        "    k_corrected = k - ((k-1)**2)/(n-1)\n",
        "\n",
        "    # Avoid division by zero\n",
        "    min_denom = min(r_corrected - 1, k_corrected - 1)\n",
        "    if np.isclose(min_denom, 0):\n",
        "        return 1.0 # Perfect correlation for this edge case\n",
        "\n",
        "    return np.sqrt(phi2_corrected / min_denom)\n",
        "\n",
        "feature_cols_only = [col for col in imputed_feature_cols if col != 'extern_tuid']\n",
        "\n",
        "\n",
        "# --- 5. GENERATE & FILTER CORRELATION MATRIX ---\n",
        "\n",
        "print(\"Step 4/4: Calculating and filtering Cramer's V Matrix...\")\n",
        "\n",
        "# Initialize matrix\n",
        "cramer_matrix = pd.DataFrame(index=feature_cols_only, columns=feature_cols_only, dtype=float)\n",
        "\n",
        "# Calculate V\n",
        "for i in range(len(feature_cols_only)):\n",
        "    for j in range(i, len(feature_cols_only)):\n",
        "        col1 = feature_cols_only[i]\n",
        "        col2 = feature_cols_only[j]\n",
        "\n",
        "        if i == j:\n",
        "            cramer_matrix.loc[col1, col2] = 1.0\n",
        "        else:\n",
        "            # Cast all features to string for safety before calculating V\n",
        "            v = cramers_v(df_sample[col1].astype(str), df_sample[col2].astype(str))\n",
        "            cramer_matrix.loc[col1, col2] = v\n",
        "            cramer_matrix.loc[col2, col1] = v\n",
        "\n",
        "\n",
        "# Identify highly redundant pairs (V > 0.95)\n",
        "HIGH_CORRELATION_THRESHOLD = 0.95\n",
        "redundant_pairs = []\n",
        "\n",
        "for i in range(len(feature_cols_only)):\n",
        "    for j in range(i + 1, len(feature_cols_only)):\n",
        "        score = cramer_matrix.iloc[i, j]\n",
        "        if score > HIGH_CORRELATION_THRESHOLD:\n",
        "            redundant_pairs.append((score, feature_cols_only[i], feature_cols_only[j]))\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "if redundant_pairs:\n",
        "    print(\"üö® HIGHLY REDUNDANT FEATURE PAIRS (Cramer's V > 0.95):\")\n",
        "    for score, col_a, col_b in sorted(redundant_pairs, reverse=True):\n",
        "        print(f\"  V={score:.4f}: {col_a} vs {col_b}\")\n",
        "\n",
        "    print(\"\\nACTION: For each pair, choose one to remove from your final training list.\")\n",
        "else:\n",
        "    print(\"‚úÖ NO HIGHLY REDUNDANT FEATURE PAIRS (V > 0.95) FOUND.\")\n",
        "\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "id": "tDlIVdsR0mUc"
      },
      "id": "tDlIVdsR0mUc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CRAMER V FOR TARGETED COLUMNS**"
      ],
      "metadata": {
        "id": "d66EYGJgCVzn"
      },
      "id": "d66EYGJgCVzn"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from scipy.stats import chi2_contingency\n",
        "import numpy as np\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "PROJECT_ID = 'stgw-shared-ai-dev'\n",
        "DATASET_ID = 'parquet_data_set'\n",
        "SAMPLE_TABLE = 'IMP_profile_FE_SAMPLE'\n",
        "\n",
        "# Initialize the BigQuery client\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "# --- 1. DEFINE CRAMER'S V FUNCTION ---\n",
        "def cramers_v(x, y):\n",
        "    \"\"\"Calculate Cramer's V for two categorical series.\"\"\"\n",
        "    # NOTE: pd.crosstab is the line that generates the memory warning/crash\n",
        "    confusion_matrix = pd.crosstab(x, y)\n",
        "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
        "    n = confusion_matrix.sum().sum()\n",
        "    r, k = confusion_matrix.shape\n",
        "\n",
        "    # Calculate Cramer's V (with bias correction)\n",
        "    phi2 = chi2 / n\n",
        "    phi2_corrected = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
        "    r_corrected = r - ((r-1)**2)/(n-1)\n",
        "    k_corrected = k - ((k-1)**2)/(n-1)\n",
        "\n",
        "    min_denom = min(r_corrected - 1, k_corrected - 1)\n",
        "    if np.isclose(min_denom, 0):\n",
        "        return 1.0\n",
        "\n",
        "    return np.sqrt(phi2_corrected / min_denom)\n",
        "\n",
        "\n",
        "# --- 2. DEFINE FEATURE LISTS (Cleaned for Training) ---\n",
        "\n",
        "TARGETED_FEATURE_LIST_CLEAN = [\n",
        "    'mortgage_loan_imputed', 'credit_cards_used_most_imputed', 'auto_imputed',\n",
        "    'auto_loan_imputed', 'Unlock_gender_imputed', 'credit_tier_imputed',\n",
        "\n",
        "    # Grocery Items (22 Remaining)\n",
        "    'grocery_tortilla_chips_imputed', 'grocery_cold_cereal_imputed', 'grocery_hot_cereal_imputed',\n",
        "    'grocery_ice_cream_imputed', 'grocery_tea_imputed', 'grocery_food_snack_dessert_imputed',\n",
        "    'grocery_mustard_imputed', 'grocery_beverages_nonalcoholic_drinks_imputed',\n",
        "    'grocery_rice_imputed', 'grocery_potato_chips_imputed', 'grocery_chocolate_candy_imputed',\n",
        "    'grocery_peanut_butter_imputed', 'grocery_pasta_imputed', 'grocery_nutritional_snacks_imputed',\n",
        "    'grocery_breads_imputed', 'grocery_iced_tea_imputed',\n",
        "    'grocery_stores_imputed', 'grocery_bbq_seasoning_sauces_imputed', 'grocery_soft_drinks_imputed',\n",
        "    'grocery_canned_soups_imputed', 'grocery_hot_dogs_imputed', 'grocery_crackers_imputed',\n",
        "\n",
        "    # Financial/Credit Features (17 Remaining)\n",
        "    'family_income_decile_imputed', 'average_household_income_imputed',\n",
        "    'financial_beliefs_imputed', 'loan_imputed', 'life_insurance_imputed',\n",
        "    'credit_behavior_imputed', 'credit_card_spend_imputed',\n",
        "    'credit_card_balance_imputed', 'adults_income_imputed', 'equity_balance_imputed',\n",
        "    'family_level_of_investable_assets_imputed', 'net_worth_imputed',\n",
        "    'home_equity_imputed', 'investments_type_imputed', 'charitable_contributions_imputed',\n",
        "    'credit_cards_used_30days_imputed', 'card_type_imputed',\n",
        "\n",
        "    # Technology/Sites (19 Remaining)\n",
        "    'tech_learning_imputed', 'streaming_system_device_use_imputed',\n",
        "    'retail_sites_imputed', 'batteries_imputed', 'tv_brand_owned_imputed',\n",
        "    'hh_cable_subscriptions_imputed', 'cell_smartphones_tablets_imputed', 'downloaded_streamed_from_internet_imputed',\n",
        "    'camera_brand_owned_imputed', 'news_sites_imputed', 'technology_imputed',\n",
        "    'residential_isp_imputed', 'social_sites_imputed',\n",
        "    'tv_when_acquired_imputed', 'tablets_imputed',\n",
        "    'sports_sites_imputed', 'travel_sites_imputed', 'mobile_service_provider_imputed', 'radio_imputed',\n",
        "    'i_device_types_imputed', 'i_iab_categories_imputed', 'i_device_oss_imputed', # Re-added i_device_oss_imputed to keep one OS feature\n",
        "    'cable_sat_provider_imputed'\n",
        "]\n",
        "\n",
        "# The list used for loading data (includes the ID)\n",
        "TARGETED_FEATURE_LIST_LOAD = ['extern_tuid'] + TARGETED_FEATURE_LIST_CLEAN\n",
        "\n",
        "\n",
        "# --- 3. LOAD TARGETED DATA ---\n",
        "column_list_str = \",\\n\".join(TARGETED_FEATURE_LIST_LOAD)\n",
        "\n",
        "sample_load_query = f\"\"\"\n",
        "SELECT {column_list_str}\n",
        "FROM `{PROJECT_ID}.{DATASET_ID}.{SAMPLE_TABLE}`\n",
        "\"\"\"\n",
        "\n",
        "print(\"Loading extended targeted data for Cramer's V check...\")\n",
        "# Load the data\n",
        "df_target = client.query(sample_load_query).to_dataframe()\n",
        "print(f\"Loaded {len(df_target)} rows with {len(TARGETED_FEATURE_LIST_CLEAN)} features.\")\n",
        "\n",
        "\n",
        "# --- 4. GENERATE & FILTER CORRELATION MATRIX ---\n",
        "feature_cols_only = TARGETED_FEATURE_LIST_CLEAN\n",
        "HIGH_CORRELATION_THRESHOLD = 0.90\n",
        "redundant_pairs = []\n",
        "\n",
        "print(f\"Calculating targeted Cramer's V scores (approx. {len(feature_cols_only)*(len(feature_cols_only)-1)//2} pairs)...\")\n",
        "\n",
        "# Monitor your memory and runtime carefully during this loop!\n",
        "for i in range(len(feature_cols_only)):\n",
        "    for j in range(i + 1, len(feature_cols_only)):\n",
        "        col1 = feature_cols_only[i]\n",
        "        col2 = feature_cols_only[j]\n",
        "\n",
        "        # Calculate V score (This is the high-memory operation)\n",
        "        v_score = cramers_v(df_target[col1].astype(str), df_target[col2].astype(str))\n",
        "\n",
        "        if v_score > HIGH_CORRELATION_THRESHOLD:\n",
        "            redundant_pairs.append((v_score, col1, col2))\n",
        "\n",
        "# --- 5. OUTPUT RESULTS ---\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "if redundant_pairs:\n",
        "    print(f\"üö® HIGHLY REDUNDANT FEATURE PAIRS (Cramer's V > {HIGH_CORRELATION_THRESHOLD:.2f}):\")\n",
        "    for score, col_a, col_b in sorted(redundant_pairs, reverse=True):\n",
        "        print(f\"  V={score:.4f}: {col_a} vs {col_b}\")\n",
        "\n",
        "    print(\"\\nACTION: For each pair, choose one to remove from your final training list.\")\n",
        "else:\n",
        "    print(\"‚úÖ No highly redundant pairs (V > 0.90) found in the extended targeted list.\")\n",
        "\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "Bn3m3-jQ8OlG"
      },
      "id": "Bn3m3-jQ8OlG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Final Training Query (Once Parameters are Tuned)\n",
        "Once you are satisfied with the performance and configuration of your sampled model, you can run the full job on the entire 261M rows by simply removing the sampling clause:"
      ],
      "metadata": {
        "id": "dqn75lZbnAue"
      },
      "id": "dqn75lZbnAue"
    },
    {
      "cell_type": "code",
      "source": [
        "FINAL_BEHAVIORAL_FEATURE_LIST_50 = [\n",
        "    'credit_tier_imputed', 'credit_card_details_imputed', 'family_income_decile_imputed',\n",
        "    'average_household_income_imputed', 'credit_card_spend_imputed', 'equity_balance_imputed',\n",
        "    'net_worth_imputed', 'credit_cards_used_most_imputed', 'discretionary_spend_imputed',\n",
        "    'tv_brand_owned_imputed', 'camera_brand_owned_imputed', 'i_location_brands_imputed',\n",
        "    'gas_brand_imputed', 'vehicle_make_imputed', 'drink_liquor_brand_imputed',\n",
        "    'mobile_phone_brand_by_carrier_imputed', 'purchase_beer_brand_imputed',\n",
        "    'purchase_wine_brand_imputed', 'drink_beer_brand_imputed', 'credit_cards_company_imputed',\n",
        "    'shoppers_apparel_purchase_last_12_mnths_imputed', 'cruises_imputed',\n",
        "    'fast_food_and_drive_in_restaurant_imputed', 'leisure_activities_hobbies_imputed',\n",
        "    'auto_parts_auto_repair_imputed', 'clothing_imputed', 'magazines_imputed',\n",
        "    'books_imputed', 'shopping_imputed', 'travel_imputed', 'hotels_for_any_travel_imputed',\n",
        "    'commerce_retail_shopping_imputed', 'family_restaurant_and_steak_house_imputed',\n",
        "    'department_discount_warehouse_imputed', 'mail_purchases_imputed', 'shoes_imputed',\n",
        "    'television_sites_imputed', 'streaming_system_device_use_imputed', 'retail_sites_imputed',\n",
        "    'entertainment_sites_imputed', 'news_sites_imputed', 'travel_sites_imputed',\n",
        "    'social_sites_imputed', 'grocery_tortilla_chips_imputed', 'grocery_cold_cereal_imputed',\n",
        "    'grocery_ice_cream_imputed', 'grocery_tea_imputed', 'grocery_food_snack_dessert_imputed',\n",
        "    'grocery_chocolate_candy_imputed', 'grocery_stores_imputed'\n",
        "]\n",
        "\n",
        "# --- CONSTRUCT THE FINAL SELECT CLAUSE (With CAST) ---\n",
        "feature_select_clauses = [\"extern_tuid\"]\n",
        "for col in FINAL_BEHAVIORAL_FEATURE_LIST_50:\n",
        "    feature_select_clauses.append(f\"CAST({col} AS STRING) AS {col}\")\n",
        "\n",
        "clean_select_statement_50 = \",\\n\".join(feature_select_clauses)\n",
        "\n",
        "print(\"Starting Autoencoder training with 50 highly-focused behavioral features...\")\n",
        "\n",
        "bqml_training_query_50 = f\"\"\"\n",
        "CREATE OR REPLACE MODEL\n",
        "  `stgw-shared-ai-dev.parquet_data_set.behavioral_autoencoder_50F`\n",
        "OPTIONS(\n",
        "  model_type='AUTOENCODER',\n",
        "  HIDDEN_UNITS=[100, 50, 100],\n",
        "  OPTIMIZER='ADAM',\n",
        "  MAX_ITERATIONS=1,\n",
        "  AUTO_CATEGORY_HASH=TRUE\n",
        ") AS\n",
        "SELECT\n",
        "    {clean_select_statement_50}\n",
        "FROM\n",
        "    `stgw-shared-ai-dev.parquet_data_set.IMP_profile_FE_SAMPLE`;\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "BWB0gAQjKG2x"
      },
      "id": "BWB0gAQjKG2x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "B. Create the Numeric Feature Matrix\n",
        "Use the trained model to generate the new, reduced features for every row."
      ],
      "metadata": {
        "id": "TuMuKO4eQEJQ"
      },
      "id": "TuMuKO4eQEJQ"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "# Assuming client is initialized and Autoencoder training was successful.\n",
        "PROJECT_ID = 'stgw-shared-ai-dev'\n",
        "LOCATION = 'us-west1'\n",
        "client = bigquery.Client(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "# **IMPORTANT: We use extern_tuid as the join key**\n",
        "bqml_generate_query = \"\"\"\n",
        "CREATE OR REPLACE TABLE\n",
        "  `stgw-shared-ai-dev.parquet_data_set.final_feature_matrix` AS\n",
        "SELECT\n",
        "  t1.extern_tuid, -- Use the correct ID column from the source table (t1)\n",
        "  t2.feature_vector AS embeddings\n",
        "FROM\n",
        "  `stgw-shared-ai-dev.parquet_data_set.IMP_profile_FE_step_2` AS t1\n",
        "JOIN\n",
        "  ML.GENERATE_EMBEDDING(\n",
        "    MODEL `stgw-shared-ai-dev.parquet_data_set.behavioral_autoencoder`,\n",
        "    (SELECT * EXCEPT(extern_tuid, created_date) FROM `stgw-shared-ai-dev.parquet_data_set.IMP_profile_FE_step_2`)\n",
        "  ) AS t2\n",
        "ON t1.extern_tuid = t2.extern_tuid;\n",
        "\"\"\"\n",
        "\n",
        "print(\"Starting BigQuery job to generate feature embeddings...\")\n",
        "job = client.query(bqml_generate_query)\n",
        "job.result()\n",
        "print(f\"Feature matrix created successfully at: {job.destination.table_id}\")"
      ],
      "metadata": {
        "id": "nnAaVtdhQMXi"
      },
      "id": "nnAaVtdhQMXi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Correlation Plot and Clustering (Colab/Scikit-learn)\n",
        "A. Import the Reduced Feature Matrix\n",
        "We import the small final_feature_matrix table and convert the single embeddings column (which contains an array of 50 latent features) into a standard DataFrame for scikit-learn."
      ],
      "metadata": {
        "id": "xPd0kz-8SqDe"
      },
      "id": "xPd0kz-8SqDe"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# Re-initialize client for clarity\n",
        "client = bigquery.Client()\n",
        "final_table_id = \"stgw-shared-ai-dev.parquet_data_set.final_feature_matrix\"\n",
        "\n",
        "# Import the final, reduced feature matrix\n",
        "df_features = client.query(f\"SELECT * FROM `{final_table_id}`\").to_dataframe()\n",
        "\n",
        "# Convert the 'embeddings' list/array column into a DataFrame of individual features\n",
        "X = pd.DataFrame(df_features['embeddings'].to_list(), index=df_features.index)\n",
        "extern_tuids = df_features['extern_tuid'] # Store IDs separately\n",
        "\n",
        "print(f\"Imported feature matrix shape: {X.shape}\")"
      ],
      "metadata": {
        "id": "kk7yyMEWStzz"
      },
      "id": "kk7yyMEWStzz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "B. Correlation Plot\n",
        "Visualize the relationships between the 50 learned latent features."
      ],
      "metadata": {
        "id": "6JOq2rtHS3Ir"
      },
      "id": "6JOq2rtHS3Ir"
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "corr_matrix = X.corr()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm')\n",
        "plt.title('Correlation Plot of 50 Latent Feature Embeddings')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gfLUrJyuS4Wj"
      },
      "id": "gfLUrJyuS4Wj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "C. Meaningful Clustering (K-Means)We perform K-Means clustering after determining the optimal $K$ (clusters) via the Elbow or Silhouette methods (not shown here, but necessary in practice)."
      ],
      "metadata": {
        "id": "Kuw4mLIaTCja"
      },
      "id": "Kuw4mLIaTCja"
    },
    {
      "cell_type": "code",
      "source": [
        "K = 5 # Example: Assume 5 clusters is optimal\n",
        "kmeans = KMeans(n_clusters=K, random_state=42, n_init='auto')\n",
        "cluster_labels = kmeans.fit_predict(X)\n",
        "\n",
        "# Attach clusters back to the IDs\n",
        "df_results = pd.DataFrame({\n",
        "    'extern_tuid': extern_tuids,\n",
        "    'cluster': cluster_labels\n",
        "})\n",
        "\n",
        "print(f\"Clustering complete. Results shape: {df_results.shape}\")\n",
        "print(df_results.head())"
      ],
      "metadata": {
        "id": "E_0dimnvTFyu"
      },
      "id": "E_0dimnvTFyu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. SHAP and Interpretation (BigQuery & Colab)A. SHAP (Feature Importance/Contribution)We treat the cluster label as a target variable and train a classifier (Random Forest) on the reduced features ($X$) to see which features drive the cluster assignment."
      ],
      "metadata": {
        "id": "ziKkediYTSPk"
      },
      "id": "ziKkediYTSPk"
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# 1. Train a classifier to predict the cluster label\n",
        "classifier = RandomForestClassifier(random_state=42)\n",
        "classifier.fit(X, cluster_labels)\n",
        "\n",
        "# 2. Calculate SHAP values for the classifier\n",
        "explainer = shap.TreeExplainer(classifier)\n",
        "shap_values = explainer.shap_values(X)\n",
        "\n",
        "# 3. Plot summary (shows which of the 50 latent features are most important)\n",
        "print(\"\\nSHAP Summary Plot:\")\n",
        "# shap_values is a list of arrays (one for each class/cluster), we plot the absolute mean\n",
        "shap.summary_plot(shap_values, X, plot_type=\"bar\")"
      ],
      "metadata": {
        "id": "uk8EgngBTTca"
      },
      "id": "uk8EgngBTTca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "B. Meaningful Interpretation (Profiling in BigQuery)\n",
        "This is the most critical step for business understanding. We push the cluster results back to BigQuery and join them to the original data to profile the clusters based on the original categories.\n",
        "\n",
        "Upload Results to BigQuery:"
      ],
      "metadata": {
        "id": "ZQak-bywTb-f"
      },
      "id": "ZQak-bywTb-f"
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the cluster assignments to a new BigQuery table\n",
        "table_id_results = \"stgw-shared-ai-dev.parquet_data_set.final_cluster_assignments\"\n",
        "job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
        "\n",
        "client.load_table_from_dataframe(\n",
        "    df_results, table_id_results, job_config=job_config\n",
        ").result()\n",
        "\n",
        "print(f\"Cluster assignments uploaded to: {table_id_results}\")"
      ],
      "metadata": {
        "id": "1UxeQvoaTenP"
      },
      "id": "1UxeQvoaTenP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Profile Clusters (Example Query):\n",
        "\n",
        "This query must be run in BigQuery (via Python) for each cluster and for multiple key original columns to define the behavioral patterns."
      ],
      "metadata": {
        "id": "9vna30nwThhL"
      },
      "id": "9vna30nwThhL"
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Find the most frequent category for a specific original column\n",
        "# (e.g., 'country') within Cluster 1.\n",
        "\n",
        "cluster_profile_query = \"\"\"\n",
        "SELECT\n",
        "  t2.country, -- Use an original column from your source data\n",
        "  COUNT(t2.country) AS count_in_cluster,\n",
        "  COUNT(t2.country) / SUM(COUNT(*)) OVER () AS proportion_in_cluster\n",
        "FROM\n",
        "  `stgw-shared-ai-dev.parquet_data_set.final_cluster_assignments` AS t1\n",
        "JOIN\n",
        "  `stgw-shared-ai-dev.parquet_data_set.IMP_profile_joined_behavioral_base` AS t2\n",
        "ON\n",
        "  t1.extern_tuid = t2.extern_tuid\n",
        "WHERE t1.cluster = 1\n",
        "GROUP BY 1\n",
        "ORDER BY count_in_cluster DESC\n",
        "LIMIT 5\n",
        "\"\"\"\n",
        "\n",
        "df_profile = client.query(cluster_profile_query).to_dataframe()\n",
        "print(\"\\nCluster 1 Profile (Top 5 countries):\")\n",
        "print(df_profile)"
      ],
      "metadata": {
        "id": "beFsoylRTwyQ"
      },
      "id": "beFsoylRTwyQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CORRRELATION**"
      ],
      "metadata": {
        "id": "17gPFnUtbry3"
      },
      "id": "17gPFnUtbry3"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from google.cloud import bigquery\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# --- Configuration ---\n",
        "PROJECT_ID = 'stgw-shared-ai-dev'\n",
        "DATASET_ID = 'parquet_data_set'\n",
        "SAMPLE_TABLE = 'IMP_profile_FE_SAMPLE'\n",
        "\n",
        "# Initialize BigQuery Client\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "print(f\"BigQuery Client initialized for project: {PROJECT_ID}\")\n",
        "\n",
        "# --- 1. Define the 9 Features to Analyze ---\n",
        "# This list combines your 3 'kept' features with 6 logical 'dropped' candidates\n",
        "# to demonstrate redundancy.\n",
        "features_to_analyze = [\n",
        "    # 16 Kept Features (from core_financial_cols)\n",
        "    'mortgage_loan_imputed',\n",
        "    'credit_tier_imputed',\n",
        "    'card_type_imputed',\n",
        "    # 6 Dropped/Redundant Candidates\n",
        "    'mortgage_imputed',            # Redundant with mortgage_loan_imputed\n",
        "    'credit_cards_imputed',        # Redundant with credit_cards_used_most/30days\n",
        "    'discretionary_spend_imputed', # Redundant with credit_card_spend/balance\n",
        "    'wealth_rating_imputed',       # Redundant with net_worth/family_level_of_investable_assets\n",
        "    'credit_behavior_imputed',     # Redundant with credit_tier\n",
        "    'personal_finance_profile_imputed' # Redundant with financial_beliefs/credit_behavior\n",
        "]\n",
        "\n",
        "column_list_str = \",\\n\".join(features_to_analyze)\n",
        "\n",
        "# --- 2. Define Cram√©r's V Function ---\n",
        "def cramers_v(x, y):\n",
        "    \"\"\"Calculates Cram√©r's V, a measure of association between two nominal variables.\"\"\"\n",
        "    confusion_matrix = pd.crosstab(x, y)\n",
        "    if confusion_matrix.empty or confusion_matrix.shape[0] == 1 or confusion_matrix.shape[1] == 1:\n",
        "        return 0.0\n",
        "\n",
        "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
        "    n = confusion_matrix.sum().sum()\n",
        "    r, k = confusion_matrix.shape\n",
        "\n",
        "    phi2 = chi2 / n\n",
        "    # Bias correction for categorical variables\n",
        "    phi2_corrected = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
        "    r_corrected = max(1, r - ((r-1)**2)/(n-1))\n",
        "    k_corrected = max(1, k - ((k-1)**2)/(n-1))\n",
        "\n",
        "    v = np.sqrt(phi2_corrected / min(r_corrected, k_corrected))\n",
        "    return min(1.0, max(0.0, v))\n",
        "\n",
        "# --- 3. Load Data from BigQuery ---\n",
        "print(f\"Loading data for {len(features_to_analyze)} features from BigQuery table: {SAMPLE_TABLE}...\")\n",
        "\n",
        "# Add a LIMIT clause for safety/speed if you want to sample, but running on full table is ideal.\n",
        "sample_load_query = f\"\"\"\n",
        "SELECT\n",
        "    {column_list_str}\n",
        "FROM\n",
        "    `{PROJECT_ID}.{DATASET_ID}.{SAMPLE_TABLE}`\n",
        "\"\"\"\n",
        "\n",
        "# Fetch the data into a Pandas DataFrame\n",
        "try:\n",
        "    df_original = client.query(sample_load_query).to_dataframe()\n",
        "    print(f\"Data loaded successfully with {len(df_original)} rows.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading data from BigQuery: {e}\")\n",
        "    # Create a dummy DataFrame if load fails, to show the heatmap structure\n",
        "    df_original = pd.DataFrame(index=np.arange(1), columns=features_to_analyze)\n",
        "\n",
        "\n",
        "# --- 4. Calculate Cram√©r's V Matrix ---\n",
        "print(\"Calculating Cram√©r's V Matrix from original data...\")\n",
        "v_matrix_original = pd.DataFrame(index=features_to_analyze,\n",
        "                                 columns=features_to_analyze,\n",
        "                                 dtype=float)\n",
        "\n",
        "for i in range(len(features_to_analyze)):\n",
        "    for j in range(i, len(features_to_analyze)):\n",
        "        col1 = features_to_analyze[i]\n",
        "        col2 = features_to_analyze[j]\n",
        "\n",
        "        # Ensure data is treated as categorical (string type)\n",
        "        v = cramers_v(df_original[col1].astype(str), df_original[col2].astype(str))\n",
        "        v_matrix_original.loc[col1, col2] = v\n",
        "        v_matrix_original.loc[col2, col1] = v # Symmetric matrix\n",
        "\n",
        "# --- 5. Generate and Save Heatmap ---\n",
        "plt.figure(figsize=(18, 16))\n",
        "sns.heatmap(\n",
        "    v_matrix_original,\n",
        "    annot=True,      # Show V values on the heatmap\n",
        "    fmt=\".2f\",       # Format annotations to 2 decimal places\n",
        "    cmap='viridis',  # Color map (dark blue for low, yellow for high)\n",
        "    linewidths=.5,   # Lines between cells\n",
        "    linecolor='white',\n",
        "    square=True,\n",
        "    vmin=0, vmax=1.0\n",
        ")\n",
        "plt.title(\"Cram√©r's V Heatmap: Redundancy Check on 9 Financial Features\", fontsize=16)\n",
        "plt.yticks(rotation=0, fontsize=10)\n",
        "plt.xticks(rotation=90, fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.savefig('cramers_v_heatmap_9_features.png')\n",
        "plt.close() # Close plot to prevent showing in environment if not needed\n",
        "\n",
        "# --- 6. Output Statistical Table ---\n",
        "print(\"\\n--- Statistical Matrix (Cram√©r's V) for 9 Features ---\")\n",
        "print(\"High values (V >= 0.75) indicate strong redundancy between the two features.\")\n",
        "print(v_matrix_original.round(2))"
      ],
      "metadata": {
        "id": "Jolc0MeFbwbw"
      },
      "id": "Jolc0MeFbwbw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FINANCE RELATED COLUMN CHOOSEN BASED ON CRAMER's V**"
      ],
      "metadata": {
        "id": "kdpi_pr4iOhU"
      },
      "id": "kdpi_pr4iOhU"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from google.cloud import bigquery\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# --- Configuration ---\n",
        "PROJECT_ID = 'stgw-shared-ai-dev'\n",
        "DATASET_ID = 'parquet_data_set'\n",
        "SAMPLE_TABLE = 'IMP_profile_FE_SAMPLE'\n",
        "\n",
        "# **CRITICAL SAFETY MEASURE: Sample Size**\n",
        "# To prevent memory issues/crashes due to high cardinality, we sample the data.\n",
        "# Start with a small number (e.g., 50,000) and increase if your runtime can handle it.\n",
        "N_SAMPLE = 50000\n",
        "\n",
        "# Initialize BigQuery Client\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "print(f\"BigQuery Client initialized for project: {PROJECT_ID}\")\n",
        "\n",
        "# --- 1. Define the 9 Features to Analyze ---\n",
        "# This list is designed to test for redundancy between Kept (3) and Dropped (6) features.\n",
        "features_to_analyze = [\n",
        "    # 3 Kept Features\n",
        "    'mortgage_loan_imputed',\n",
        "    'credit_tier_imputed',\n",
        "    'card_type_imputed',\n",
        "    # 6 Dropped/Redundant Candidates\n",
        "    'mortgage_imputed',\n",
        "    'credit_cards_imputed',\n",
        "    'discretionary_spend_imputed',\n",
        "    'wealth_rating_imputed',\n",
        "    'credit_behavior_imputed',\n",
        "    'personal_finance_profile_imputed'\n",
        "]\n",
        "\n",
        "column_list_str = \",\\n    \".join(features_to_analyze)\n",
        "\n",
        "# --- 2. Define Cram√©r's V Function ---\n",
        "def cramers_v(x, y):\n",
        "    \"\"\"Calculates Cram√©r's V, a measure of association between two nominal variables.\"\"\"\n",
        "    # Ensure all values are strings to treat as categories\n",
        "    x, y = x.astype(str), y.astype(str)\n",
        "\n",
        "    confusion_matrix = pd.crosstab(x, y)\n",
        "    if confusion_matrix.empty or confusion_matrix.shape[0] == 1 or confusion_matrix.shape[1] == 1:\n",
        "        return 0.0\n",
        "\n",
        "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
        "    n = confusion_matrix.sum().sum()\n",
        "    r, k = confusion_matrix.shape\n",
        "\n",
        "    phi2 = chi2 / n\n",
        "    # Bias correction for categorical variables\n",
        "    phi2_corrected = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
        "    r_corrected = max(1, r - ((r-1)**2)/(n-1))\n",
        "    k_corrected = max(1, k - ((k-1)**2)/(n-1))\n",
        "\n",
        "    v = np.sqrt(phi2_corrected / min(r_corrected, k_corrected))\n",
        "    return min(1.0, max(0.0, v))\n",
        "\n",
        "# --- 3. Load Data from BigQuery (with Sampling) ---\n",
        "print(f\"Loading a random sample of {N_SAMPLE} rows for {len(features_to_analyze)} features from BigQuery table: {SAMPLE_TABLE}...\")\n",
        "\n",
        "# Use a two-stage process: query all data and sample in Pandas, OR sample directly in BigQuery\n",
        "# For simplicity, we use the BigQuery sample function if available or a LIMIT:\n",
        "sample_load_query = f\"\"\"\n",
        "SELECT\n",
        "    {column_list_str}\n",
        "FROM\n",
        "    `{PROJECT_ID}.{DATASET_ID}.{SAMPLE_TABLE}`\n",
        "WHERE\n",
        "    rand() < {N_SAMPLE / 1300000.0} -- Approximate percentage based on 1.3M total rows\n",
        "LIMIT {N_SAMPLE}\n",
        "\"\"\"\n",
        "\n",
        "# Fetch the data into a Pandas DataFrame\n",
        "try:\n",
        "    df_original = client.query(sample_load_query).to_dataframe()\n",
        "    # Final check on sample size\n",
        "    if len(df_original) < N_SAMPLE * 0.8:\n",
        "        print(f\"Warning: Loaded {len(df_original)} rows, less than target sample size.\")\n",
        "    else:\n",
        "        print(f\"Data loaded successfully with {len(df_original)} rows.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading data from BigQuery: {e}\")\n",
        "    # Exit if data load fails\n",
        "    print(\"Cannot proceed without data. Please check your BigQuery connection and table path.\")\n",
        "    raise\n",
        "\n",
        "# --- 4. Calculate Cram√©r's V Matrix ---\n",
        "print(\"Calculating Cram√©r's V Matrix from sampled data...\")\n",
        "v_matrix_original = pd.DataFrame(index=features_to_analyze,\n",
        "                                 columns=features_to_analyze,\n",
        "                                 dtype=float)\n",
        "\n",
        "for i in range(len(features_to_analyze)):\n",
        "    for j in range(i, len(features_to_analyze)):\n",
        "        col1 = features_to_analyze[i]\n",
        "        col2 = features_to_analyze[j]\n",
        "\n",
        "        v = cramers_v(df_original[col1], df_original[col2])\n",
        "        v_matrix_original.loc[col1, col2] = v\n",
        "        v_matrix_original.loc[col2, col1] = v # Symmetric matrix\n",
        "\n",
        "\n",
        "# --- 5. Generate and Save Heatmap ---\n",
        "plt.figure(figsize=(10, 8)) # Smaller size for 9 features\n",
        "sns.heatmap(\n",
        "    v_matrix_original.astype(float),\n",
        "    annot=True,\n",
        "    fmt=\".2f\",\n",
        "    cmap='viridis',\n",
        "    linewidths=.5,\n",
        "    linecolor='white',\n",
        "    square=True,\n",
        "    vmin=0, vmax=1.0\n",
        ")\n",
        "plt.title(\"Cram√©r's V Heatmap: Redundancy Check on 9 Financial Features\", fontsize=12)\n",
        "plt.yticks(rotation=0, fontsize=9)\n",
        "plt.xticks(rotation=90, fontsize=9)\n",
        "plt.tight_layout()\n",
        "plt.savefig('cramers_v_heatmap_9_features_final.png')\n",
        "plt.close()\n",
        "\n",
        "# --- 6. Output Statistical Table ---\n",
        "print(\"\\ncramers_v_heatmap_9_features_final.png saved.\")\n",
        "print(\"\\n--- Statistical Matrix (Cram√©r's V) for 9 Features ---\")\n",
        "print(\"High values (V >= 0.75) indicate strong redundancy (yellow cells).\")\n",
        "print(v_matrix_original.round(2))"
      ],
      "metadata": {
        "id": "k_UmHe9nbyfg"
      },
      "id": "k_UmHe9nbyfg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Check on 9 HNI-Type Behavior Features**"
      ],
      "metadata": {
        "id": "h0UiZzPLigGi"
      },
      "id": "h0UiZzPLigGi"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from google.cloud import bigquery\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# --- Configuration ---\n",
        "PROJECT_ID = 'stgw-shared-ai-dev'\n",
        "DATASET_ID = 'parquet_data_set'\n",
        "SAMPLE_TABLE = 'IMP_profile_FE_SAMPLE'\n",
        "N_SAMPLE = 50000\n",
        "TOTAL_ROWS_EST = 1300000\n",
        "\n",
        "# Initialize BigQuery Client\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "print(f\"BigQuery Client initialized for project: {PROJECT_ID}\")\n",
        "\n",
        "# --- 1. Define the 9 Features to Analyze (HNI-Type Behavior) ---\n",
        "features_to_analyze = [\n",
        "    # 4 Kept Features\n",
        "    'travel_sites_imputed',\n",
        "    'mobile_service_provider_imputed',\n",
        "    'movie_genre_saw_in_theater_imputed',\n",
        "    'number_of_vehicles_in_household_imputed',\n",
        "    # 5 Highly Correlated Candidates to Drop\n",
        "    'travel_imputed',\n",
        "    'hotels_for_any_travel_imputed',\n",
        "    'mobile_devices_imputed',\n",
        "    'entertainment_sites_imputed',\n",
        "    'auto_imputed'\n",
        "]\n",
        "\n",
        "column_list_str = \",\\n    \".join(features_to_analyze)\n",
        "\n",
        "# --- 2. Define Cram√©r's V Function ---\n",
        "def cramers_v(x, y):\n",
        "    \"\"\"Calculates Cram√©r's V with bias correction for association between two nominal variables.\"\"\"\n",
        "    x, y = x.astype(str), y.astype(str)\n",
        "\n",
        "    confusion_matrix = pd.crosstab(x, y)\n",
        "    if confusion_matrix.empty or confusion_matrix.shape[0] <= 1 or confusion_matrix.shape[1] <= 1:\n",
        "        return 0.0\n",
        "\n",
        "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
        "    n = confusion_matrix.sum().sum()\n",
        "    r, k = confusion_matrix.shape\n",
        "\n",
        "    phi2 = chi2 / n\n",
        "    phi2_corrected = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
        "    r_corrected = max(1, r - ((r-1)**2)/(n-1))\n",
        "    k_corrected = max(1, k - ((k-1)**2)/(n-1))\n",
        "\n",
        "    v = np.sqrt(phi2_corrected / min(r_corrected, k_corrected))\n",
        "    return min(1.0, max(0.0, v))\n",
        "\n",
        "# --- 3. Load Data from BigQuery (with Sampling) ---\n",
        "print(f\"Loading a random sample of {N_SAMPLE} rows for {len(features_to_analyze)} features from BigQuery table: {SAMPLE_TABLE}...\")\n",
        "\n",
        "# Estimate the required sampling fraction\n",
        "sample_rate = N_SAMPLE / TOTAL_ROWS_EST\n",
        "if sample_rate > 1: sample_rate = 1.0\n",
        "\n",
        "sample_load_query = f\"\"\"\n",
        "SELECT\n",
        "    {column_list_str}\n",
        "FROM\n",
        "    `{PROJECT_ID}.{DATASET_ID}.{SAMPLE_TABLE}`\n",
        "WHERE\n",
        "    rand() < {sample_rate}\n",
        "LIMIT {N_SAMPLE}\n",
        "\"\"\"\n",
        "\n",
        "# Fetch the data into a Pandas DataFrame\n",
        "try:\n",
        "    df_original = client.query(sample_load_query).to_dataframe()\n",
        "    print(f\"Data loaded successfully with {len(df_original)} rows.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading data from BigQuery: {e}\")\n",
        "    print(\"Cannot proceed without data. Please check your BigQuery connection and table path.\")\n",
        "    raise\n",
        "\n",
        "# --- 4. Calculate Cram√©r's V Matrix ---\n",
        "print(\"Calculating Cram√©r's V Matrix from sampled data...\")\n",
        "v_matrix_original = pd.DataFrame(index=features_to_analyze,\n",
        "                                 columns=features_to_analyze,\n",
        "                                 dtype=float)\n",
        "\n",
        "for i in range(len(features_to_analyze)):\n",
        "    for j in range(i, len(features_to_analyze)):\n",
        "        col1 = features_to_analyze[i]\n",
        "        col2 = features_to_analyze[j]\n",
        "\n",
        "        v = cramers_v(df_original[col1], df_original[col2])\n",
        "        v_matrix_original.loc[col1, col2] = v\n",
        "        v_matrix_original.loc[col2, col1] = v # Symmetric matrix\n",
        "\n",
        "\n",
        "# --- 5. Generate and Save Heatmap ---\n",
        "FILE_NAME = 'cramers_v_heatmap_9_HNI_behavior.png'\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(\n",
        "    v_matrix_original.astype(float),\n",
        "    annot=True,\n",
        "    fmt=\".2f\",\n",
        "    cmap='viridis',\n",
        "    linewidths=.5,\n",
        "    linecolor='white',\n",
        "    square=True,\n",
        "    vmin=0, vmax=1.0\n",
        ")\n",
        "plt.title(\"Cram√©r's V Heatmap: Redundancy Check on 9 HNI-Type Behavior Features\", fontsize=12)\n",
        "plt.yticks(rotation=0, fontsize=9)\n",
        "plt.xticks(rotation=90, fontsize=9)\n",
        "plt.tight_layout()\n",
        "plt.savefig(FILE_NAME)\n",
        "plt.close()\n",
        "\n",
        "# --- 6. Output Statistical Table ---\n",
        "print(f\"\\n{FILE_NAME} saved.\")\n",
        "print(\"\\n--- Statistical Matrix (Cram√©r's V) for 9 HNI-Type Behavior Features ---\")\n",
        "print(\"High values (V >= 0.75) indicate strong redundancy (yellow cells).\")\n",
        "print(v_matrix_original.round(2))"
      ],
      "metadata": {
        "id": "5pnXpSE5ie3Y"
      },
      "id": "5pnXpSE5ie3Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **# Finding Zero Correlation**"
      ],
      "metadata": {
        "id": "c7EAiC56vTuM"
      },
      "id": "c7EAiC56vTuM"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from google.cloud import bigquery\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# --- Configuration ---\n",
        "PROJECT_ID = 'stgw-shared-ai-dev'\n",
        "DATASET_ID = 'parquet_data_set'\n",
        "SAMPLE_TABLE = 'IMP_profile_FE_SAMPLE'\n",
        "N_SAMPLE = 50000\n",
        "TOTAL_ROWS_EST = 1300000\n",
        "\n",
        "# Initialize BigQuery Client\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "print(f\"BigQuery Client initialized for project: {PROJECT_ID}\")\n",
        "\n",
        "# --- 1. Define the 16 Features to Analyze (Core & Financial) ---\n",
        "features_to_analyze = [\n",
        "    'mortgage_loan_imputed',\n",
        "    'credit_cards_used_most_imputed',\n",
        "    'auto_loan_imputed',\n",
        "    'Unlock_gender_imputed',\n",
        "    'credit_tier_imputed',\n",
        "    'family_income_decile_imputed',\n",
        "    'average_household_income_imputed',\n",
        "    'financial_beliefs_imputed',\n",
        "    'loan_imputed',\n",
        "    'credit_card_spend_imputed',\n",
        "    'credit_card_balance_imputed',\n",
        "    'equity_balance_imputed',\n",
        "    'family_level_of_investable_assets_imputed',\n",
        "    'net_worth_imputed',\n",
        "     'discretionary_spend_imputed',\n",
        "    'wealth_rating_imputed',\n",
        "    'credit_cards_used_30days_imputed',\n",
        "    'card_type_imputed'\n",
        "]\n",
        "\n",
        "column_list_str = \",\\n    \".join(features_to_analyze)\n",
        "\n",
        "# --- 2. Define Cram√©r's V Function ---\n",
        "def cramers_v(x, y):\n",
        "    \"\"\"Calculates Cram√©r's V with bias correction for association between two nominal variables.\"\"\"\n",
        "    x, y = x.astype(str), y.astype(str)\n",
        "\n",
        "    confusion_matrix = pd.crosstab(x, y)\n",
        "    if confusion_matrix.empty or confusion_matrix.shape[0] <= 1 or confusion_matrix.shape[1] <= 1:\n",
        "        return 0.0\n",
        "\n",
        "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
        "    n = confusion_matrix.sum().sum()\n",
        "    r, k = confusion_matrix.shape\n",
        "\n",
        "    phi2 = chi2 / n\n",
        "    phi2_corrected = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
        "    r_corrected = max(1, r - ((r-1)**2)/(n-1))\n",
        "    k_corrected = max(1, k - ((k-1)**2)/(n-1))\n",
        "\n",
        "    v = np.sqrt(phi2_corrected / min(r_corrected, k_corrected))\n",
        "    return min(1.0, max(0.0, v))\n",
        "\n",
        "# --- 3. Load Data from BigQuery (with Sampling) ---\n",
        "print(f\"Loading a random sample of {N_SAMPLE} rows for {len(features_to_analyze)} features from BigQuery table: {SAMPLE_TABLE}...\")\n",
        "\n",
        "# Estimate the required sampling fraction\n",
        "sample_rate = N_SAMPLE / TOTAL_ROWS_EST\n",
        "if sample_rate > 1: sample_rate = 1.0\n",
        "\n",
        "sample_load_query = f\"\"\"\n",
        "SELECT\n",
        "    {column_list_str}\n",
        "FROM\n",
        "    `{PROJECT_ID}.{DATASET_ID}.{SAMPLE_TABLE}`\n",
        "WHERE\n",
        "    rand() < {sample_rate}\n",
        "LIMIT {N_SAMPLE}\n",
        "\"\"\"\n",
        "\n",
        "# Fetch the data into a Pandas DataFrame\n",
        "try:\n",
        "    df_original = client.query(sample_load_query).to_dataframe()\n",
        "    print(f\"Data loaded successfully with {len(df_original)} rows.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading data from BigQuery: {e}\")\n",
        "    print(\"Cannot proceed without data. Please check your BigQuery connection and table path.\")\n",
        "    raise\n",
        "\n",
        "# --- 4. Calculate Cram√©r's V Matrix ---\n",
        "print(\"Calculating Cram√©r's V Matrix from sampled data...\")\n",
        "v_matrix_original = pd.DataFrame(index=features_to_analyze,\n",
        "                                 columns=features_to_analyze,\n",
        "                                 dtype=float)\n",
        "\n",
        "for i in range(len(features_to_analyze)):\n",
        "    for j in range(i, len(features_to_analyze)):\n",
        "        col1 = features_to_analyze[i]\n",
        "        col2 = features_to_analyze[j]\n",
        "\n",
        "        v = cramers_v(df_original[col1], df_original[col2])\n",
        "        v_matrix_original.loc[col1, col2] = v\n",
        "        v_matrix_original.loc[col2, col1] = v # Symmetric matrix\n",
        "\n",
        "\n",
        "# --- 5. Generate and Save Heatmap ---\n",
        "FILE_NAME = 'cramers_v_heatmap_16_core_financial.png'\n",
        "plt.figure(figsize=(14, 12))\n",
        "sns.heatmap(\n",
        "    v_matrix_original.astype(float),\n",
        "    annot=True,\n",
        "    fmt=\".2f\",\n",
        "    cmap='viridis',\n",
        "    linewidths=.5,\n",
        "    linecolor='white',\n",
        "    square=True,\n",
        "    vmin=0, vmax=1.0\n",
        ")\n",
        "plt.title(\"Cram√©r's V Heatmap: Redundancy Check on 16 Core & Financial Features\", fontsize=14)\n",
        "plt.yticks(rotation=0, fontsize=9)\n",
        "plt.xticks(rotation=90, fontsize=9)\n",
        "plt.tight_layout()\n",
        "plt.savefig(FILE_NAME)\n",
        "plt.close()\n",
        "\n",
        "# --- 6. Output Statistical Table ---\n",
        "print(f\"\\n{FILE_NAME} saved.\")\n",
        "print(\"\\n--- Statistical Matrix (Cram√©r's V) for 16 Core & Financial Features ---\")\n",
        "print(\"High values (V >= 0.75) indicate strong redundancy (yellow cells).\")\n",
        "print(v_matrix_original.round(2))"
      ],
      "metadata": {
        "id": "SDWXcDXpvTPb"
      },
      "id": "SDWXcDXpvTPb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = \"stgw-shared-ai-dev.parquet_data_set\"\n",
        "table = \"IMP_profile_FE_SAMPLE\"\n",
        "\n",
        "columns = [\n",
        "    \"mortgage_loan_imputed\",\n",
        "    \"credit_cards_used_most_imputed\",\n",
        "    \"auto_imputed\",\n",
        "    \"auto_loan_imputed\",\n",
        "    \"Unlock_gender_imputed\",\n",
        "    \"credit_tier_imputed\",\n",
        "    \"family_income_decile_imputed\",\n",
        "    \"average_household_income_imputed\",\n",
        "    \"financial_beliefs_imputed\",\n",
        "    \"loan_imputed\",\n",
        "    \"credit_card_spend_imputed\",\n",
        "    \"credit_card_balance_imputed\",\n",
        "    \"equity_balance_imputed\",\n",
        "    \"family_level_of_investable_assets_imputed\",\n",
        "    \"net_worth_imputed\",\n",
        "    \"investments_type_imputed\",\n",
        "    \"credit_cards_used_30days_imputed\",\n",
        "    \"card_type_imputed\",\n",
        "    \"tech_learning_imputed\",\n",
        "    \"streaming_system_device_use_imputed\",\n",
        "    \"retail_sites_imputed\",\n",
        "    \"batteries_imputed\",\n",
        "    \"tv_brand_owned_imputed\",\n",
        "    \"hh_cable_subscriptions_imputed\",\n",
        "    \"cell_smartphones_tablets_imputed\",\n",
        "    \"downloaded_streamed_from_internet_imputed\",\n",
        "    \"camera_brand_owned_imputed\",\n",
        "    \"news_sites_imputed\",\n",
        "    \"technology_imputed\",\n",
        "    \"residential_isp_imputed\",\n",
        "    \"social_sites_imputed\",\n",
        "    \"tv_when_acquired_imputed\",\n",
        "    \"tablets_imputed\",\n",
        "    \"sports_sites_imputed\",\n",
        "    \"travel_sites_imputed\",\n",
        "    \"mobile_service_provider_imputed\",\n",
        "    \"radio_imputed\",\n",
        "    \"i_device_types_imputed\",\n",
        "    \"cable_sat_provider_imputed\",\n",
        "    \"movie_genre_saw_in_theater_imputed\",\n",
        "    \"number_of_vehicles_in_household_imputed\",\n",
        "    \"automotive_insurance_imputed\",\n",
        "    \"number_of_visits_imputed\",\n",
        "    \"television_sites_imputed\",\n",
        "    \"contributions_imputed\",\n",
        "    \"grocery_stores_imputed\",\n",
        "    \"grocery_soft_drinks_imputed\",\n",
        "    \"grocery_hot_dogs_imputed\",\n",
        "    \"grocery_crackers_imputed\",\n",
        "    \"charitable_contributions_imputed\"\n",
        "]\n",
        "\n",
        "case_statements = []\n",
        "for col in columns:\n",
        "    stmt = (\n",
        "        \"CASE \"\n",
        "        \"WHEN {0} IS NULL OR TRIM({0}) = '' THEN '__MISSING__' \"\n",
        "        \"WHEN {0} IN (\"\n",
        "        \"SELECT {0} FROM `{1}.{2}` \"\n",
        "        \"GROUP BY {0} \"\n",
        "        \"ORDER BY COUNT(*) DESC \"\n",
        "        \"LIMIT 50) THEN {0} \"\n",
        "        \"ELSE 'OTHER' END AS {0}\"\n",
        "    ).format(col, dataset, table)\n",
        "    case_statements.append(stmt)\n",
        "\n",
        "# Combine into final SQL\n",
        "select_clause = \",\\n\".join(case_statements)\n",
        "sql = (\n",
        "    f\"CREATE OR REPLACE TABLE `{dataset}.IMP_profile_FE_SAMPLE_TOP50` AS\\n\"\n",
        "    f\"SELECT extern_tuid,\\n{select_clause}\\n\"\n",
        "    f\"FROM `{dataset}.{table}`;\"\n",
        ")\n",
        "\n",
        "print(sql)\n"
      ],
      "metadata": {
        "id": "QPEP3_sXb_vb"
      },
      "id": "QPEP3_sXb_vb",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "Master-notebook-Data-Prep"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}